
==> Audit <==
┌─────────┬────────────────────┬──────────┬───────┬─────────┬─────────────────────┬─────────────────────┐
│ COMMAND │        ARGS        │ PROFILE  │ USER  │ VERSION │     START TIME      │      END TIME       │
├─────────┼────────────────────┼──────────┼───────┼─────────┼─────────────────────┼─────────────────────┤
│ start   │ --driver=docker    │ minikube │ larry │ v1.37.0 │ 23 Sep 25 17:08 EAT │                     │
│ start   │ --driver=docker    │ minikube │ larry │ v1.37.0 │ 23 Sep 25 18:06 EAT │ 23 Sep 25 18:06 EAT │
│ service │ django-service.yml │ minikube │ larry │ v1.37.0 │ 23 Sep 25 18:12 EAT │                     │
│ service │ django-service     │ minikube │ larry │ v1.37.0 │ 23 Sep 25 18:12 EAT │ 23 Sep 25 18:12 EAT │
│ service │ django-service     │ minikube │ larry │ v1.37.0 │ 23 Sep 25 18:18 EAT │                     │
│ service │ django-service     │ minikube │ larry │ v1.37.0 │ 23 Sep 25 18:18 EAT │                     │
└─────────┴────────────────────┴──────────┴───────┴─────────┴─────────────────────┴─────────────────────┘


==> Last Start <==
Log file created at: 2025/09/23 18:06:05
Running on machine: fendo
Binary: Built with gc go1.24.6 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0923 18:06:05.306127  457310 out.go:360] Setting OutFile to fd 1 ...
I0923 18:06:05.306263  457310 out.go:413] isatty.IsTerminal(1) = true
I0923 18:06:05.306268  457310 out.go:374] Setting ErrFile to fd 2...
I0923 18:06:05.306275  457310 out.go:413] isatty.IsTerminal(2) = true
I0923 18:06:05.306546  457310 root.go:338] Updating PATH: /home/larry/.minikube/bin
W0923 18:06:05.306713  457310 root.go:314] Error reading config file at /home/larry/.minikube/config/config.json: open /home/larry/.minikube/config/config.json: no such file or directory
I0923 18:06:05.308105  457310 out.go:368] Setting JSON to false
I0923 18:06:05.309517  457310 start.go:130] hostinfo: {"hostname":"fendo","uptime":171865,"bootTime":1758468100,"procs":276,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.8.0-83-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"99040b10-8876-41f3-9344-938445f4a9fa"}
I0923 18:06:05.309618  457310 start.go:140] virtualization: kvm host
I0923 18:06:05.319039  457310 out.go:179] 😄  minikube v1.37.0 on Ubuntu 22.04
I0923 18:06:05.335361  457310 notify.go:220] Checking for updates...
I0923 18:06:05.336287  457310 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
E0923 18:06:05.336349  457310 start.go:829] api.Load failed for minikube: filestore "minikube": Docker machine "minikube" does not exist. Use "docker-machine ls" to list machines. Use "docker-machine create" to add a new one.
I0923 18:06:05.336477  457310 driver.go:421] Setting default libvirt URI to qemu:///system
E0923 18:06:05.336518  457310 start.go:829] api.Load failed for minikube: filestore "minikube": Docker machine "minikube" does not exist. Use "docker-machine ls" to list machines. Use "docker-machine create" to add a new one.
I0923 18:06:05.376278  457310 docker.go:123] docker version: linux-25.0.0:Docker Engine - Community
I0923 18:06:05.376405  457310 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0923 18:06:05.428897  457310 info.go:266] docker info: {ID:ad23cf95-bfe6-4bd2-a10a-e2e6e1a3a03d Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:22 OomKillDisable:false NGoroutines:46 SystemTime:2025-09-23 18:06:05.418946288 +0300 EAT LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-83-generic OperatingSystem:Ubuntu 22.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:12434010112 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:fendo Labels:[] ExperimentalBuild:false ServerVersion:25.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:a1496014c916f9e62104b33d1bb5bd03b0858e59 Expected:a1496014c916f9e62104b33d1bb5bd03b0858e59} RuncCommit:{ID:v1.1.11-0-g4bccb38 Expected:v1.1.11-0-g4bccb38} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.1]] Warnings:<nil>}}
I0923 18:06:05.428989  457310 docker.go:318] overlay module found
I0923 18:06:05.438572  457310 out.go:179] ✨  Using the docker driver based on existing profile
I0923 18:06:05.445694  457310 start.go:304] selected driver: docker
I0923 18:06:05.445707  457310 start.go:918] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:2798 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0923 18:06:05.445855  457310 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0923 18:06:05.446107  457310 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0923 18:06:05.514088  457310 info.go:266] docker info: {ID:ad23cf95-bfe6-4bd2-a10a-e2e6e1a3a03d Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:22 OomKillDisable:false NGoroutines:46 SystemTime:2025-09-23 18:06:05.504053835 +0300 EAT LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-83-generic OperatingSystem:Ubuntu 22.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:12434010112 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:fendo Labels:[] ExperimentalBuild:false ServerVersion:25.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:a1496014c916f9e62104b33d1bb5bd03b0858e59 Expected:a1496014c916f9e62104b33d1bb5bd03b0858e59} RuncCommit:{ID:v1.1.11-0-g4bccb38 Expected:v1.1.11-0-g4bccb38} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.1]] Warnings:<nil>}}
I0923 18:06:05.514633  457310 cni.go:84] Creating CNI manager for ""
I0923 18:06:05.514687  457310 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0923 18:06:05.515358  457310 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:2798 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0923 18:06:05.586254  457310 out.go:179] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0923 18:06:05.587429  457310 cache.go:123] Beginning downloading kic base image for docker with docker
I0923 18:06:05.588165  457310 out.go:179] 🚜  Pulling base image v0.0.48 ...
I0923 18:06:05.589202  457310 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0923 18:06:05.589238  457310 preload.go:146] Found local preload: /home/larry/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I0923 18:06:05.589258  457310 cache.go:58] Caching tarball of preloaded images
I0923 18:06:05.589293  457310 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I0923 18:06:05.589382  457310 preload.go:172] Found /home/larry/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0923 18:06:05.589397  457310 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I0923 18:06:05.589556  457310 profile.go:143] Saving config to /home/larry/.minikube/profiles/minikube/config.json ...
I0923 18:06:05.609597  457310 cache.go:152] Downloading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I0923 18:06:05.609852  457310 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory
I0923 18:06:05.609873  457310 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory, skipping pull
I0923 18:06:05.609877  457310 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 exists in cache, skipping pull
I0923 18:06:05.609886  457310 cache.go:155] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 as a tarball
I0923 18:06:05.609890  457310 cache.go:165] Loading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from local cache
I0923 18:06:20.357148  457310 cache.go:167] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from cached tarball
I0923 18:06:20.357172  457310 cache.go:232] Successfully downloaded all kic artifacts
I0923 18:06:20.357207  457310 start.go:360] acquireMachinesLock for minikube: {Name:mkd0b70c031493ef971990b33c2b1a1b19e9d307 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0923 18:06:20.357287  457310 start.go:364] duration metric: took 56.486µs to acquireMachinesLock for "minikube"
I0923 18:06:20.357310  457310 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:2798 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0923 18:06:20.357375  457310 start.go:125] createHost starting for "" (driver="docker")
I0923 18:06:20.358280  457310 out.go:252] 🔥  Creating docker container (CPUs=2, Memory=2798MB) ...
I0923 18:06:20.359358  457310 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0923 18:06:20.359390  457310 client.go:168] LocalClient.Create starting
I0923 18:06:20.359521  457310 main.go:141] libmachine: Creating CA: /home/larry/.minikube/certs/ca.pem
I0923 18:06:20.604559  457310 main.go:141] libmachine: Creating client certificate: /home/larry/.minikube/certs/cert.pem
I0923 18:06:21.524312  457310 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0923 18:06:21.538974  457310 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0923 18:06:21.539064  457310 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0923 18:06:21.539078  457310 cli_runner.go:164] Run: docker network inspect minikube
W0923 18:06:21.554122  457310 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0923 18:06:21.554144  457310 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0923 18:06:21.554153  457310 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0923 18:06:21.554276  457310 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0923 18:06:21.571038  457310 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001771580}
I0923 18:06:21.571070  457310 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0923 18:06:21.571142  457310 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0923 18:06:21.683649  457310 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0923 18:06:21.683683  457310 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0923 18:06:21.683937  457310 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0923 18:06:21.711614  457310 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0923 18:06:21.732381  457310 oci.go:103] Successfully created a docker volume minikube
I0923 18:06:21.732486  457310 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -d /var/lib
I0923 18:06:23.543219  457310 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -d /var/lib: (1.810695565s)
I0923 18:06:23.543239  457310 oci.go:107] Successfully prepared a docker volume minikube
I0923 18:06:23.543290  457310 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0923 18:06:23.543310  457310 kic.go:194] Starting extracting preloaded images to volume ...
I0923 18:06:23.543420  457310 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/larry/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir
I0923 18:06:28.488651  457310 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/larry/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir: (4.94519906s)
I0923 18:06:28.488702  457310 kic.go:203] duration metric: took 4.945390418s to extract preloaded images to volume ...
W0923 18:06:28.488796  457310 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0923 18:06:28.488826  457310 oci.go:252] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0923 18:06:28.488889  457310 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0923 18:06:28.551714  457310 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2798mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1
I0923 18:06:28.960795  457310 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0923 18:06:28.981514  457310 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0923 18:06:28.999473  457310 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0923 18:06:29.062592  457310 oci.go:144] the created container "minikube" has a running status.
I0923 18:06:29.062614  457310 kic.go:225] Creating ssh key for kic: /home/larry/.minikube/machines/minikube/id_rsa...
I0923 18:06:29.699690  457310 kic_runner.go:191] docker (temp): /home/larry/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0923 18:06:29.762920  457310 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0923 18:06:29.802762  457310 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0923 18:06:29.802804  457310 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0923 18:06:29.902673  457310 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0923 18:06:29.945001  457310 machine.go:93] provisionDockerMachine start ...
I0923 18:06:29.945212  457310 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 18:06:29.977265  457310 main.go:141] libmachine: Using SSH client type: native
I0923 18:06:29.977675  457310 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0923 18:06:29.977686  457310 main.go:141] libmachine: About to run SSH command:
hostname
I0923 18:06:30.136974  457310 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0923 18:06:30.137017  457310 ubuntu.go:182] provisioning hostname "minikube"
I0923 18:06:30.137337  457310 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 18:06:30.162378  457310 main.go:141] libmachine: Using SSH client type: native
I0923 18:06:30.162634  457310 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0923 18:06:30.162643  457310 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0923 18:06:30.336111  457310 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0923 18:06:30.336489  457310 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 18:06:30.355871  457310 main.go:141] libmachine: Using SSH client type: native
I0923 18:06:30.356098  457310 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0923 18:06:30.356110  457310 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0923 18:06:30.492672  457310 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0923 18:06:30.492692  457310 ubuntu.go:188] set auth options {CertDir:/home/larry/.minikube CaCertPath:/home/larry/.minikube/certs/ca.pem CaPrivateKeyPath:/home/larry/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/larry/.minikube/machines/server.pem ServerKeyPath:/home/larry/.minikube/machines/server-key.pem ClientKeyPath:/home/larry/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/larry/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/larry/.minikube}
I0923 18:06:30.492714  457310 ubuntu.go:190] setting up certificates
I0923 18:06:30.492725  457310 provision.go:84] configureAuth start
I0923 18:06:30.492848  457310 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0923 18:06:30.509441  457310 provision.go:143] copyHostCerts
I0923 18:06:30.509501  457310 exec_runner.go:151] cp: /home/larry/.minikube/certs/cert.pem --> /home/larry/.minikube/cert.pem (1119 bytes)
I0923 18:06:30.509619  457310 exec_runner.go:151] cp: /home/larry/.minikube/certs/key.pem --> /home/larry/.minikube/key.pem (1675 bytes)
I0923 18:06:30.509698  457310 exec_runner.go:151] cp: /home/larry/.minikube/certs/ca.pem --> /home/larry/.minikube/ca.pem (1074 bytes)
I0923 18:06:30.509765  457310 provision.go:117] generating server cert: /home/larry/.minikube/machines/server.pem ca-key=/home/larry/.minikube/certs/ca.pem private-key=/home/larry/.minikube/certs/ca-key.pem org=larry.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0923 18:06:30.589529  457310 provision.go:177] copyRemoteCerts
I0923 18:06:30.589596  457310 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0923 18:06:30.589653  457310 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 18:06:30.605484  457310 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/larry/.minikube/machines/minikube/id_rsa Username:docker}
I0923 18:06:30.703834  457310 ssh_runner.go:362] scp /home/larry/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0923 18:06:30.742238  457310 ssh_runner.go:362] scp /home/larry/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0923 18:06:30.769158  457310 ssh_runner.go:362] scp /home/larry/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0923 18:06:30.795814  457310 provision.go:87] duration metric: took 303.027259ms to configureAuth
I0923 18:06:30.795836  457310 ubuntu.go:206] setting minikube options for container-runtime
I0923 18:06:30.796022  457310 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0923 18:06:30.796109  457310 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 18:06:30.812235  457310 main.go:141] libmachine: Using SSH client type: native
I0923 18:06:30.812461  457310 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0923 18:06:30.812469  457310 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0923 18:06:30.954122  457310 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0923 18:06:30.954134  457310 ubuntu.go:71] root file system type: overlay
I0923 18:06:30.954223  457310 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0923 18:06:30.954299  457310 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 18:06:30.971439  457310 main.go:141] libmachine: Using SSH client type: native
I0923 18:06:30.971669  457310 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0923 18:06:30.971749  457310 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0923 18:06:31.138237  457310 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I0923 18:06:31.138594  457310 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 18:06:31.157603  457310 main.go:141] libmachine: Using SSH client type: native
I0923 18:06:31.157839  457310 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0923 18:06:31.157852  457310 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0923 18:06:32.416565  457310 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-09-03 20:55:49.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-09-23 15:06:31.133799564 +0000
@@ -9,23 +9,34 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0923 18:06:32.416583  457310 machine.go:96] duration metric: took 2.471571894s to provisionDockerMachine
I0923 18:06:32.416592  457310 client.go:171] duration metric: took 12.057197629s to LocalClient.Create
I0923 18:06:32.416609  457310 start.go:167] duration metric: took 12.057253395s to libmachine.API.Create "minikube"
I0923 18:06:32.416616  457310 start.go:293] postStartSetup for "minikube" (driver="docker")
I0923 18:06:32.416623  457310 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0923 18:06:32.416696  457310 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0923 18:06:32.416756  457310 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 18:06:32.435186  457310 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/larry/.minikube/machines/minikube/id_rsa Username:docker}
I0923 18:06:32.539221  457310 ssh_runner.go:195] Run: cat /etc/os-release
I0923 18:06:32.543398  457310 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0923 18:06:32.543433  457310 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0923 18:06:32.543446  457310 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0923 18:06:32.543454  457310 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0923 18:06:32.543469  457310 filesync.go:126] Scanning /home/larry/.minikube/addons for local assets ...
I0923 18:06:32.543553  457310 filesync.go:126] Scanning /home/larry/.minikube/files for local assets ...
I0923 18:06:32.543603  457310 start.go:296] duration metric: took 126.980753ms for postStartSetup
I0923 18:06:32.544437  457310 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0923 18:06:32.567555  457310 profile.go:143] Saving config to /home/larry/.minikube/profiles/minikube/config.json ...
I0923 18:06:32.567901  457310 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0923 18:06:32.567960  457310 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 18:06:32.584360  457310 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/larry/.minikube/machines/minikube/id_rsa Username:docker}
I0923 18:06:32.679761  457310 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0923 18:06:32.686145  457310 start.go:128] duration metric: took 12.328757152s to createHost
I0923 18:06:32.686161  457310 start.go:83] releasing machines lock for "minikube", held for 12.328856779s
I0923 18:06:32.686244  457310 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0923 18:06:32.705075  457310 ssh_runner.go:195] Run: cat /version.json
I0923 18:06:32.705164  457310 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 18:06:32.705180  457310 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0923 18:06:32.705300  457310 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 18:06:32.736120  457310 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/larry/.minikube/machines/minikube/id_rsa Username:docker}
I0923 18:06:32.749475  457310 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/larry/.minikube/machines/minikube/id_rsa Username:docker}
I0923 18:06:32.847670  457310 ssh_runner.go:195] Run: systemctl --version
I0923 18:06:33.389965  457310 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0923 18:06:33.395519  457310 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0923 18:06:33.430243  457310 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0923 18:06:33.430350  457310 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0923 18:06:33.476333  457310 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0923 18:06:33.476351  457310 start.go:495] detecting cgroup driver to use...
I0923 18:06:33.476381  457310 detect.go:190] detected "systemd" cgroup driver on host os
I0923 18:06:33.476485  457310 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0923 18:06:33.495820  457310 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I0923 18:06:33.507723  457310 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0923 18:06:33.518675  457310 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0923 18:06:33.518780  457310 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0923 18:06:33.529414  457310 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0923 18:06:33.544134  457310 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0923 18:06:33.561223  457310 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0923 18:06:33.572164  457310 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0923 18:06:33.581930  457310 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0923 18:06:33.592626  457310 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0923 18:06:33.603240  457310 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0923 18:06:33.613930  457310 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0923 18:06:33.623427  457310 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0923 18:06:33.633678  457310 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0923 18:06:33.723745  457310 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0923 18:06:33.806939  457310 start.go:495] detecting cgroup driver to use...
I0923 18:06:33.806987  457310 detect.go:190] detected "systemd" cgroup driver on host os
I0923 18:06:33.807074  457310 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0923 18:06:33.822242  457310 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0923 18:06:33.833925  457310 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0923 18:06:33.860813  457310 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0923 18:06:33.873133  457310 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0923 18:06:33.886716  457310 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0923 18:06:33.905265  457310 ssh_runner.go:195] Run: which cri-dockerd
I0923 18:06:33.908670  457310 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0923 18:06:33.922187  457310 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I0923 18:06:33.947379  457310 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0923 18:06:34.045652  457310 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0923 18:06:34.132113  457310 docker.go:575] configuring docker to use "systemd" as cgroup driver...
I0923 18:06:34.132211  457310 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0923 18:06:34.163940  457310 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0923 18:06:34.175593  457310 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0923 18:06:34.261170  457310 ssh_runner.go:195] Run: sudo systemctl restart docker
I0923 18:06:35.111585  457310 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I0923 18:06:35.124025  457310 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0923 18:06:35.136624  457310 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0923 18:06:35.152932  457310 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0923 18:06:35.252042  457310 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0923 18:06:35.339379  457310 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0923 18:06:35.481833  457310 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0923 18:06:35.523138  457310 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0923 18:06:35.545270  457310 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0923 18:06:35.727360  457310 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0923 18:06:35.862146  457310 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0923 18:06:35.881245  457310 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0923 18:06:35.881417  457310 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0923 18:06:35.886081  457310 start.go:563] Will wait 60s for crictl version
I0923 18:06:35.886159  457310 ssh_runner.go:195] Run: which crictl
I0923 18:06:35.891203  457310 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0923 18:06:35.936197  457310 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I0923 18:06:35.936283  457310 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0923 18:06:35.960620  457310 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0923 18:06:35.991296  457310 out.go:252] 🐳  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I0923 18:06:35.991643  457310 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0923 18:06:36.016295  457310 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0923 18:06:36.024117  457310 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0923 18:06:36.045543  457310 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:2798 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0923 18:06:36.045659  457310 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0923 18:06:36.045757  457310 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0923 18:06:36.075566  457310 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0923 18:06:36.075579  457310 docker.go:621] Images already preloaded, skipping extraction
I0923 18:06:36.075682  457310 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0923 18:06:36.108466  457310 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0923 18:06:36.108477  457310 cache_images.go:85] Images are preloaded, skipping loading
I0923 18:06:36.108485  457310 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I0923 18:06:36.108575  457310 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0923 18:06:36.108663  457310 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0923 18:06:36.171972  457310 cni.go:84] Creating CNI manager for ""
I0923 18:06:36.172024  457310 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0923 18:06:36.172045  457310 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0923 18:06:36.172082  457310 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0923 18:06:36.172286  457310 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0923 18:06:36.172425  457310 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I0923 18:06:36.184322  457310 binaries.go:44] Found k8s binaries, skipping transfer
I0923 18:06:36.184423  457310 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0923 18:06:36.196742  457310 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0923 18:06:36.216567  457310 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0923 18:06:36.236659  457310 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I0923 18:06:36.257067  457310 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0923 18:06:36.263217  457310 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0923 18:06:36.283748  457310 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0923 18:06:36.369468  457310 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0923 18:06:36.397244  457310 certs.go:68] Setting up /home/larry/.minikube/profiles/minikube for IP: 192.168.49.2
I0923 18:06:36.397256  457310 certs.go:194] generating shared ca certs ...
I0923 18:06:36.397275  457310 certs.go:226] acquiring lock for ca certs: {Name:mk53f52d889ae4eeb7019051aecba70364f83604 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0923 18:06:36.397366  457310 certs.go:240] generating "minikubeCA" ca cert: /home/larry/.minikube/ca.key
I0923 18:06:37.073352  457310 crypto.go:156] Writing cert to /home/larry/.minikube/ca.crt ...
I0923 18:06:37.073379  457310 lock.go:35] WriteFile acquiring /home/larry/.minikube/ca.crt: {Name:mk87c3963579e2fe6cfd03db14391ae493f363d6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0923 18:06:37.073660  457310 crypto.go:164] Writing key to /home/larry/.minikube/ca.key ...
I0923 18:06:37.073670  457310 lock.go:35] WriteFile acquiring /home/larry/.minikube/ca.key: {Name:mk4092d0db5aba81fcbca7b844697961be13fd2f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0923 18:06:37.073785  457310 certs.go:240] generating "proxyClientCA" ca cert: /home/larry/.minikube/proxy-client-ca.key
I0923 18:06:37.510107  457310 crypto.go:156] Writing cert to /home/larry/.minikube/proxy-client-ca.crt ...
I0923 18:06:37.510126  457310 lock.go:35] WriteFile acquiring /home/larry/.minikube/proxy-client-ca.crt: {Name:mk3bc72533c1d559170ffdc5ab666581fea373a5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0923 18:06:37.510318  457310 crypto.go:164] Writing key to /home/larry/.minikube/proxy-client-ca.key ...
I0923 18:06:37.510326  457310 lock.go:35] WriteFile acquiring /home/larry/.minikube/proxy-client-ca.key: {Name:mkfbb754048a07b148a1aa52955ccdaeafa6225a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0923 18:06:37.510423  457310 certs.go:256] generating profile certs ...
I0923 18:06:37.510469  457310 certs.go:363] generating signed profile cert for "minikube-user": /home/larry/.minikube/profiles/minikube/client.key
I0923 18:06:37.510479  457310 crypto.go:68] Generating cert /home/larry/.minikube/profiles/minikube/client.crt with IP's: []
I0923 18:06:37.853104  457310 crypto.go:156] Writing cert to /home/larry/.minikube/profiles/minikube/client.crt ...
I0923 18:06:37.853117  457310 lock.go:35] WriteFile acquiring /home/larry/.minikube/profiles/minikube/client.crt: {Name:mkf0d8bcf18c5ec784be0d4f62f88b19d6d0692e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0923 18:06:37.853255  457310 crypto.go:164] Writing key to /home/larry/.minikube/profiles/minikube/client.key ...
I0923 18:06:37.853262  457310 lock.go:35] WriteFile acquiring /home/larry/.minikube/profiles/minikube/client.key: {Name:mk870aa4085ef2301ee0c0f0b4a6c09c35d74561 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0923 18:06:37.853339  457310 certs.go:363] generating signed profile cert for "minikube": /home/larry/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0923 18:06:37.853357  457310 crypto.go:68] Generating cert /home/larry/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0923 18:06:38.185891  457310 crypto.go:156] Writing cert to /home/larry/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0923 18:06:38.185930  457310 lock.go:35] WriteFile acquiring /home/larry/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk97019de6653d785d2713b877ff1dad49601d03 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0923 18:06:38.186125  457310 crypto.go:164] Writing key to /home/larry/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0923 18:06:38.186134  457310 lock.go:35] WriteFile acquiring /home/larry/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk1b815b9491a0dc3558096c2139a6096ad3b6c8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0923 18:06:38.186222  457310 certs.go:381] copying /home/larry/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/larry/.minikube/profiles/minikube/apiserver.crt
I0923 18:06:38.186296  457310 certs.go:385] copying /home/larry/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/larry/.minikube/profiles/minikube/apiserver.key
I0923 18:06:38.186348  457310 certs.go:363] generating signed profile cert for "aggregator": /home/larry/.minikube/profiles/minikube/proxy-client.key
I0923 18:06:38.186362  457310 crypto.go:68] Generating cert /home/larry/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0923 18:06:38.315124  457310 crypto.go:156] Writing cert to /home/larry/.minikube/profiles/minikube/proxy-client.crt ...
I0923 18:06:38.315138  457310 lock.go:35] WriteFile acquiring /home/larry/.minikube/profiles/minikube/proxy-client.crt: {Name:mk8bd2aac4c46033f0e7200a54c35680cc8da14f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0923 18:06:38.315283  457310 crypto.go:164] Writing key to /home/larry/.minikube/profiles/minikube/proxy-client.key ...
I0923 18:06:38.315291  457310 lock.go:35] WriteFile acquiring /home/larry/.minikube/profiles/minikube/proxy-client.key: {Name:mkf4fde5f9c75aa031e5038f18a05f63132cc528 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0923 18:06:38.315472  457310 certs.go:484] found cert: /home/larry/.minikube/certs/ca-key.pem (1679 bytes)
I0923 18:06:38.315500  457310 certs.go:484] found cert: /home/larry/.minikube/certs/ca.pem (1074 bytes)
I0923 18:06:38.315520  457310 certs.go:484] found cert: /home/larry/.minikube/certs/cert.pem (1119 bytes)
I0923 18:06:38.315542  457310 certs.go:484] found cert: /home/larry/.minikube/certs/key.pem (1675 bytes)
I0923 18:06:38.316084  457310 ssh_runner.go:362] scp /home/larry/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0923 18:06:38.342536  457310 ssh_runner.go:362] scp /home/larry/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0923 18:06:38.372301  457310 ssh_runner.go:362] scp /home/larry/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0923 18:06:38.404175  457310 ssh_runner.go:362] scp /home/larry/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0923 18:06:38.430685  457310 ssh_runner.go:362] scp /home/larry/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0923 18:06:38.455776  457310 ssh_runner.go:362] scp /home/larry/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0923 18:06:38.491703  457310 ssh_runner.go:362] scp /home/larry/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0923 18:06:38.519121  457310 ssh_runner.go:362] scp /home/larry/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0923 18:06:38.545289  457310 ssh_runner.go:362] scp /home/larry/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0923 18:06:38.579611  457310 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0923 18:06:38.602891  457310 ssh_runner.go:195] Run: openssl version
I0923 18:06:38.608574  457310 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0923 18:06:38.620998  457310 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0923 18:06:38.625427  457310 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Sep 23 15:06 /usr/share/ca-certificates/minikubeCA.pem
I0923 18:06:38.625511  457310 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0923 18:06:38.632856  457310 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0923 18:06:38.643372  457310 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0923 18:06:38.647337  457310 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0923 18:06:38.647379  457310 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:2798 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0923 18:06:38.647496  457310 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0923 18:06:38.664321  457310 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0923 18:06:38.677883  457310 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0923 18:06:38.692028  457310 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0923 18:06:38.692154  457310 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0923 18:06:38.703452  457310 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0923 18:06:38.703469  457310 kubeadm.go:157] found existing configuration files:

I0923 18:06:38.703549  457310 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0923 18:06:38.714502  457310 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0923 18:06:38.714620  457310 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0923 18:06:38.726453  457310 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0923 18:06:38.739536  457310 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0923 18:06:38.739661  457310 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0923 18:06:38.754133  457310 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0923 18:06:38.764817  457310 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0923 18:06:38.764899  457310 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0923 18:06:38.774915  457310 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0923 18:06:38.785753  457310 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0923 18:06:38.785891  457310 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0923 18:06:38.798101  457310 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0923 18:06:38.863539  457310 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0923 18:06:38.868718  457310 kubeadm.go:310] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/6.8.0-83-generic\n", err: exit status 1
I0923 18:06:38.927164  457310 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0923 18:06:56.612409  457310 kubeadm.go:310] [init] Using Kubernetes version: v1.34.0
I0923 18:06:56.612496  457310 kubeadm.go:310] [preflight] Running pre-flight checks
I0923 18:06:56.612634  457310 kubeadm.go:310] [preflight] The system verification failed. Printing the output from the verification:
I0923 18:06:56.612725  457310 kubeadm.go:310] [0;37mKERNEL_VERSION[0m: [0;32m6.8.0-83-generic[0m
I0923 18:06:56.612787  457310 kubeadm.go:310] [0;37mOS[0m: [0;32mLinux[0m
I0923 18:06:56.612859  457310 kubeadm.go:310] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I0923 18:06:56.612964  457310 kubeadm.go:310] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I0923 18:06:56.613041  457310 kubeadm.go:310] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I0923 18:06:56.613198  457310 kubeadm.go:310] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I0923 18:06:56.613273  457310 kubeadm.go:310] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I0923 18:06:56.613347  457310 kubeadm.go:310] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I0923 18:06:56.613422  457310 kubeadm.go:310] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I0923 18:06:56.613490  457310 kubeadm.go:310] [0;37mCGROUPS_IO[0m: [0;32menabled[0m
I0923 18:06:56.613604  457310 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0923 18:06:56.613753  457310 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0923 18:06:56.614264  457310 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0923 18:06:56.614401  457310 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0923 18:06:56.618153  457310 out.go:252]     ▪ Generating certificates and keys ...
I0923 18:06:56.618389  457310 kubeadm.go:310] [certs] Using existing ca certificate authority
I0923 18:06:56.618498  457310 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0923 18:06:56.618605  457310 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0923 18:06:56.618697  457310 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0923 18:06:56.618798  457310 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0923 18:06:56.618877  457310 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0923 18:06:56.618962  457310 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0923 18:06:56.619141  457310 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0923 18:06:56.619225  457310 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0923 18:06:56.620732  457310 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0923 18:06:56.620934  457310 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0923 18:06:56.621163  457310 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0923 18:06:56.621244  457310 kubeadm.go:310] [certs] Generating "sa" key and public key
I0923 18:06:56.621392  457310 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0923 18:06:56.621478  457310 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0923 18:06:56.621641  457310 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0923 18:06:56.621749  457310 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0923 18:06:56.621895  457310 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0923 18:06:56.621987  457310 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0923 18:06:56.622124  457310 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0923 18:06:56.622235  457310 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0923 18:06:56.623684  457310 out.go:252]     ▪ Booting up control plane ...
I0923 18:06:56.629064  457310 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0923 18:06:56.629239  457310 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0923 18:06:56.629346  457310 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0923 18:06:56.629510  457310 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0923 18:06:56.629657  457310 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/instance-config.yaml"
I0923 18:06:56.630164  457310 kubeadm.go:310] [patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
I0923 18:06:56.630302  457310 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0923 18:06:56.630364  457310 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0923 18:06:56.630572  457310 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0923 18:06:56.630738  457310 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0923 18:06:56.630841  457310 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.004063673s
I0923 18:06:56.630987  457310 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0923 18:06:56.631115  457310 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I0923 18:06:56.631257  457310 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0923 18:06:56.631560  457310 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0923 18:06:56.631824  457310 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 6.023536696s
I0923 18:06:56.632368  457310 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 6.550673519s
I0923 18:06:56.632600  457310 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 8.501525175s
I0923 18:06:56.632779  457310 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0923 18:06:56.632979  457310 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0923 18:06:56.633070  457310 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0923 18:06:56.633382  457310 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0923 18:06:56.633471  457310 kubeadm.go:310] [bootstrap-token] Using token: zz4d3i.yi6h772eaquzhz1t
I0923 18:06:56.636898  457310 out.go:252]     ▪ Configuring RBAC rules ...
I0923 18:06:56.638425  457310 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0923 18:06:56.638574  457310 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0923 18:06:56.638820  457310 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0923 18:06:56.639060  457310 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0923 18:06:56.639254  457310 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0923 18:06:56.639939  457310 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0923 18:06:56.640257  457310 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0923 18:06:56.640331  457310 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0923 18:06:56.640408  457310 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0923 18:06:56.640413  457310 kubeadm.go:310] 
I0923 18:06:56.640514  457310 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0923 18:06:56.640518  457310 kubeadm.go:310] 
I0923 18:06:56.640648  457310 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0923 18:06:56.640653  457310 kubeadm.go:310] 
I0923 18:06:56.640694  457310 kubeadm.go:310]   mkdir -p $HOME/.kube
I0923 18:06:56.640824  457310 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0923 18:06:56.640910  457310 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0923 18:06:56.640914  457310 kubeadm.go:310] 
I0923 18:06:56.641004  457310 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0923 18:06:56.641009  457310 kubeadm.go:310] 
I0923 18:06:56.641088  457310 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0923 18:06:56.641093  457310 kubeadm.go:310] 
I0923 18:06:56.641180  457310 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0923 18:06:56.641306  457310 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0923 18:06:56.641421  457310 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0923 18:06:56.641432  457310 kubeadm.go:310] 
I0923 18:06:56.643185  457310 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0923 18:06:56.643438  457310 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0923 18:06:56.643446  457310 kubeadm.go:310] 
I0923 18:06:56.643575  457310 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token zz4d3i.yi6h772eaquzhz1t \
I0923 18:06:56.643740  457310 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:c83c25940c909b7384e1fdf52d885efeb23d41befee1f9c7930baaa941aeb200 \
I0923 18:06:56.643817  457310 kubeadm.go:310] 	--control-plane 
I0923 18:06:56.643823  457310 kubeadm.go:310] 
I0923 18:06:56.643959  457310 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0923 18:06:56.643964  457310 kubeadm.go:310] 
I0923 18:06:56.644095  457310 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token zz4d3i.yi6h772eaquzhz1t \
I0923 18:06:56.644274  457310 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:c83c25940c909b7384e1fdf52d885efeb23d41befee1f9c7930baaa941aeb200 
I0923 18:06:56.644284  457310 cni.go:84] Creating CNI manager for ""
I0923 18:06:56.644302  457310 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0923 18:06:56.652106  457310 out.go:179] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0923 18:06:56.652964  457310 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0923 18:06:56.691973  457310 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0923 18:06:56.734938  457310 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_09_23T18_06_56_0700 minikube.k8s.io/version=v1.37.0 minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0923 18:06:56.736146  457310 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0923 18:06:56.742470  457310 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0923 18:06:57.161522  457310 ops.go:34] apiserver oom_adj: -16
I0923 18:06:57.161566  457310 kubeadm.go:1105] duration metric: took 425.709491ms to wait for elevateKubeSystemPrivileges
I0923 18:06:57.161833  457310 kubeadm.go:394] duration metric: took 18.514455564s to StartCluster
I0923 18:06:57.161854  457310 settings.go:142] acquiring lock: {Name:mk3cff8a388ac6f41c2d850a4b3ceefcc48dcc56 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0923 18:06:57.161921  457310 settings.go:150] Updating kubeconfig:  /home/larry/.kube/config
I0923 18:06:57.168970  457310 lock.go:35] WriteFile acquiring /home/larry/.kube/config: {Name:mk97ae59428da9d7fb439fba455c3195da9cd52d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0923 18:06:57.169267  457310 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0923 18:06:57.169372  457310 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0923 18:06:57.169735  457310 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0923 18:06:57.169840  457310 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0923 18:06:57.169863  457310 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0923 18:06:57.169891  457310 host.go:66] Checking if "minikube" exists ...
I0923 18:06:57.169891  457310 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0923 18:06:57.169915  457310 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0923 18:06:57.169926  457310 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0923 18:06:57.170535  457310 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0923 18:06:57.173105  457310 out.go:179] 🔎  Verifying Kubernetes components...
I0923 18:06:57.173675  457310 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0923 18:06:57.174140  457310 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0923 18:06:57.259898  457310 out.go:179]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0923 18:06:57.260940  457310 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0923 18:06:57.260967  457310 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0923 18:06:57.261214  457310 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 18:06:57.297324  457310 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0923 18:06:57.297364  457310 host.go:66] Checking if "minikube" exists ...
I0923 18:06:57.298095  457310 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0923 18:06:57.359911  457310 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/larry/.minikube/machines/minikube/id_rsa Username:docker}
I0923 18:06:57.407021  457310 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0923 18:06:57.407035  457310 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0923 18:06:57.407173  457310 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 18:06:57.449724  457310 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/larry/.minikube/machines/minikube/id_rsa Username:docker}
I0923 18:06:57.515246  457310 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0923 18:06:57.579277  457310 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0923 18:06:57.676213  457310 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0923 18:06:57.786710  457310 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0923 18:06:58.319881  457310 start.go:976] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I0923 18:06:58.325805  457310 api_server.go:52] waiting for apiserver process to appear ...
I0923 18:06:58.325923  457310 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0923 18:06:58.668888  457310 api_server.go:72] duration metric: took 1.499592364s to wait for apiserver process to appear ...
I0923 18:06:58.668904  457310 api_server.go:88] waiting for apiserver healthz status ...
I0923 18:06:58.668924  457310 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0923 18:06:58.680594  457310 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0923 18:06:58.683858  457310 api_server.go:141] control plane version: v1.34.0
I0923 18:06:58.683879  457310 api_server.go:131] duration metric: took 14.968041ms to wait for apiserver health ...
I0923 18:06:58.683888  457310 system_pods.go:43] waiting for kube-system pods to appear ...
I0923 18:06:58.686545  457310 out.go:179] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0923 18:06:58.691120  457310 addons.go:514] duration metric: took 1.521430315s for enable addons: enabled=[storage-provisioner default-storageclass]
I0923 18:06:58.699715  457310 system_pods.go:59] 5 kube-system pods found
I0923 18:06:58.699745  457310 system_pods.go:61] "etcd-minikube" [0be582a5-ca59-44c0-bcf7-b77820450a58] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0923 18:06:58.699756  457310 system_pods.go:61] "kube-apiserver-minikube" [aa9e35f3-0e25-45d5-af57-ec8105c3d8f4] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0923 18:06:58.699768  457310 system_pods.go:61] "kube-controller-manager-minikube" [5cc1fdb5-e416-4453-b3b6-11ce44b45b1e] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0923 18:06:58.699783  457310 system_pods.go:61] "kube-scheduler-minikube" [00c51cdb-849a-420e-a84b-795aabdf47b2] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0923 18:06:58.699789  457310 system_pods.go:61] "storage-provisioner" [729ed539-68c5-4f82-8d35-3dd524f62a0b] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. no new claims to deallocate, preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0923 18:06:58.699796  457310 system_pods.go:74] duration metric: took 15.901473ms to wait for pod list to return data ...
I0923 18:06:58.699810  457310 kubeadm.go:578] duration metric: took 1.530518495s to wait for: map[apiserver:true system_pods:true]
I0923 18:06:58.699828  457310 node_conditions.go:102] verifying NodePressure condition ...
I0923 18:06:58.705402  457310 node_conditions.go:122] node storage ephemeral capacity is 49586760Ki
I0923 18:06:58.705429  457310 node_conditions.go:123] node cpu capacity is 4
I0923 18:06:58.705791  457310 node_conditions.go:105] duration metric: took 5.955831ms to run NodePressure ...
I0923 18:06:58.705807  457310 start.go:241] waiting for startup goroutines ...
I0923 18:06:58.831384  457310 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0923 18:06:58.831460  457310 start.go:246] waiting for cluster config update ...
I0923 18:06:58.831479  457310 start.go:255] writing updated cluster config ...
I0923 18:06:58.832843  457310 ssh_runner.go:195] Run: rm -f paused
I0923 18:06:59.153185  457310 start.go:617] kubectl: 1.34.1, cluster: 1.34.0 (minor skew: 0)
I0923 18:06:59.153951  457310 out.go:179] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Sep 23 15:06:34 minikube dockerd[1093]: time="2025-09-23T15:06:34.401118522Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Sep 23 15:06:34 minikube dockerd[1093]: time="2025-09-23T15:06:34.432881669Z" level=info msg="Loading containers: start."
Sep 23 15:06:35 minikube dockerd[1093]: time="2025-09-23T15:06:35.020975641Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 7c064713ce0aa0a8e55611fe4b904006ab6161357fec82b1bb2cce20b120a006], retrying...."
Sep 23 15:06:35 minikube dockerd[1093]: time="2025-09-23T15:06:35.067807024Z" level=info msg="Loading containers: done."
Sep 23 15:06:35 minikube dockerd[1093]: time="2025-09-23T15:06:35.082515690Z" level=info msg="Docker daemon" commit=249d679 containerd-snapshotter=false storage-driver=overlay2 version=28.4.0
Sep 23 15:06:35 minikube dockerd[1093]: time="2025-09-23T15:06:35.082574816Z" level=info msg="Initializing buildkit"
Sep 23 15:06:35 minikube dockerd[1093]: time="2025-09-23T15:06:35.103353000Z" level=info msg="Completed buildkit initialization"
Sep 23 15:06:35 minikube dockerd[1093]: time="2025-09-23T15:06:35.108574057Z" level=info msg="Daemon has completed initialization"
Sep 23 15:06:35 minikube dockerd[1093]: time="2025-09-23T15:06:35.108646312Z" level=info msg="API listen on /var/run/docker.sock"
Sep 23 15:06:35 minikube dockerd[1093]: time="2025-09-23T15:06:35.108848752Z" level=info msg="API listen on /run/docker.sock"
Sep 23 15:06:35 minikube dockerd[1093]: time="2025-09-23T15:06:35.108904521Z" level=info msg="API listen on [::]:2376"
Sep 23 15:06:35 minikube systemd[1]: Started Docker Application Container Engine.
Sep 23 15:06:35 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Sep 23 15:06:35 minikube cri-dockerd[1394]: time="2025-09-23T15:06:35Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Sep 23 15:06:35 minikube cri-dockerd[1394]: time="2025-09-23T15:06:35Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Sep 23 15:06:35 minikube cri-dockerd[1394]: time="2025-09-23T15:06:35Z" level=info msg="Start docker client with request timeout 0s"
Sep 23 15:06:35 minikube cri-dockerd[1394]: time="2025-09-23T15:06:35Z" level=info msg="Hairpin mode is set to hairpin-veth"
Sep 23 15:06:35 minikube cri-dockerd[1394]: time="2025-09-23T15:06:35Z" level=info msg="Loaded network plugin cni"
Sep 23 15:06:35 minikube cri-dockerd[1394]: time="2025-09-23T15:06:35Z" level=info msg="Docker cri networking managed by network plugin cni"
Sep 23 15:06:35 minikube cri-dockerd[1394]: time="2025-09-23T15:06:35Z" level=info msg="Setting cgroupDriver systemd"
Sep 23 15:06:35 minikube cri-dockerd[1394]: time="2025-09-23T15:06:35Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Sep 23 15:06:35 minikube cri-dockerd[1394]: time="2025-09-23T15:06:35Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Sep 23 15:06:35 minikube cri-dockerd[1394]: time="2025-09-23T15:06:35Z" level=info msg="Start cri-dockerd grpc backend"
Sep 23 15:06:35 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Sep 23 15:06:47 minikube cri-dockerd[1394]: time="2025-09-23T15:06:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d1e1811c58b12a8b887692558642a3e21537d12026a09fd06207ee231bd5ca05/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Sep 23 15:06:47 minikube cri-dockerd[1394]: time="2025-09-23T15:06:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/08a50441eead82df95e9ac01874b76b05edff403a86046b3c394b17c7bbcf430/resolv.conf as [nameserver 192.168.49.1 options trust-ad ndots:0 edns0]"
Sep 23 15:06:47 minikube cri-dockerd[1394]: time="2025-09-23T15:06:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d81666a0f79cd5b887a0b499b5fdcc78cd486b53057e15469f9d7ad0f649cf75/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Sep 23 15:06:47 minikube cri-dockerd[1394]: time="2025-09-23T15:06:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/aa03cce3493835ebbbae1d980d232f8cbc98d18c6071989655ecb4a3ea748597/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Sep 23 15:07:01 minikube cri-dockerd[1394]: time="2025-09-23T15:07:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c2c62a1f166e6ffa3b84c08069ea83e5f5233265b26f2f538406a97097c64fe5/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Sep 23 15:07:01 minikube dockerd[1093]: time="2025-09-23T15:07:01.550682692Z" level=info msg="ignoring event" container=0d23b8fdab0dc8418aea2bdbce33efda58c6063b407c603a177572479ca99f47 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 23 15:07:01 minikube cri-dockerd[1394]: time="2025-09-23T15:07:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/156a3db6c13073ca332bcb7810c01edabad490249564676a426e7576ed945f67/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Sep 23 15:07:01 minikube cri-dockerd[1394]: time="2025-09-23T15:07:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a6cbbffc8e9af350e06073846b3f5641e5d66b1b42da32bf9a7460334d10cd5f/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Sep 23 15:07:06 minikube cri-dockerd[1394]: time="2025-09-23T15:07:06Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Sep 23 15:07:21 minikube dockerd[1093]: time="2025-09-23T15:07:21.462276950Z" level=info msg="ignoring event" container=398f9370eca69920c1721cc653986263f651f17449c8bd0c75fc4ef717104bdb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 23 15:08:00 minikube cri-dockerd[1394]: time="2025-09-23T15:08:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e67b420c8a20e540b00b41679f10218b87fdb6a5c367d3764ef89709394f1ee2/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 23 15:08:03 minikube dockerd[1093]: time="2025-09-23T15:08:03.698403031Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 23 15:08:03 minikube dockerd[1093]: time="2025-09-23T15:08:03.698472333Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 23 15:08:11 minikube cri-dockerd[1394]: time="2025-09-23T15:08:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bf43c46bd3e17cce6f8f7bbfbaf782c19dc5d1f0c68cebba830cc4a6c2bc5d6c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 23 15:08:13 minikube dockerd[1093]: time="2025-09-23T15:08:13.955401262Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 23 15:08:13 minikube dockerd[1093]: time="2025-09-23T15:08:13.955510948Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 23 15:08:18 minikube dockerd[1093]: time="2025-09-23T15:08:18.218069731Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 23 15:08:18 minikube dockerd[1093]: time="2025-09-23T15:08:18.218131922Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 23 15:08:32 minikube dockerd[1093]: time="2025-09-23T15:08:32.308457817Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 23 15:08:32 minikube dockerd[1093]: time="2025-09-23T15:08:32.308514579Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 23 15:08:44 minikube dockerd[1093]: time="2025-09-23T15:08:44.232531365Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 23 15:08:44 minikube dockerd[1093]: time="2025-09-23T15:08:44.232582462Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 23 15:09:04 minikube dockerd[1093]: time="2025-09-23T15:09:04.224702476Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 23 15:09:04 minikube dockerd[1093]: time="2025-09-23T15:09:04.224788258Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 23 15:09:31 minikube dockerd[1093]: time="2025-09-23T15:09:31.196336520Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 23 15:09:31 minikube dockerd[1093]: time="2025-09-23T15:09:31.196389242Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 23 15:09:55 minikube dockerd[1093]: time="2025-09-23T15:09:55.218733577Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 23 15:09:55 minikube dockerd[1093]: time="2025-09-23T15:09:55.218810781Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 23 15:11:03 minikube dockerd[1093]: time="2025-09-23T15:11:03.223596953Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 23 15:11:03 minikube dockerd[1093]: time="2025-09-23T15:11:03.223657951Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 23 15:11:29 minikube dockerd[1093]: time="2025-09-23T15:11:29.297127561Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 23 15:11:29 minikube dockerd[1093]: time="2025-09-23T15:11:29.297177764Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 23 15:13:58 minikube dockerd[1093]: time="2025-09-23T15:13:58.332173511Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 23 15:13:58 minikube dockerd[1093]: time="2025-09-23T15:13:58.332240556Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 23 15:14:18 minikube dockerd[1093]: time="2025-09-23T15:14:18.286256722Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 23 15:14:18 minikube dockerd[1093]: time="2025-09-23T15:14:18.286314544Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
aa60f6b4899f9       6e38f40d628db       11 minutes ago      Running             storage-provisioner       2                   c2c62a1f166e6       storage-provisioner
8807c0f49e72f       52546a367cc9e       11 minutes ago      Running             coredns                   0                   a6cbbffc8e9af       coredns-66bc5c9577-hs9b4
b773c2b8df2bd       df0860106674d       11 minutes ago      Running             kube-proxy                0                   156a3db6c1307       kube-proxy-dk8cf
398f9370eca69       6e38f40d628db       11 minutes ago      Exited              storage-provisioner       1                   c2c62a1f166e6       storage-provisioner
874a089d62cd6       46169d968e920       12 minutes ago      Running             kube-scheduler            0                   aa03cce349383       kube-scheduler-minikube
32419c28ac4c0       a0af72f2ec6d6       12 minutes ago      Running             kube-controller-manager   0                   d81666a0f79cd       kube-controller-manager-minikube
fd29ba9de78cb       90550c43ad2bc       12 minutes ago      Running             kube-apiserver            0                   08a50441eead8       kube-apiserver-minikube
5ba7f6f422ea9       5f1f5298c888d       12 minutes ago      Running             etcd                      0                   d1e1811c58b12       etcd-minikube


==> coredns [8807c0f49e72] <==
maxprocs: Leaving GOMAXPROCS=4: CPU quota undefined
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:39640 - 52605 "HINFO IN 6971715386424386032.3814270949275236679. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.428013389s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_09_23T18_06_56_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 23 Sep 2025 15:06:53 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 23 Sep 2025 15:18:50 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 23 Sep 2025 15:14:14 +0000   Tue, 23 Sep 2025 15:06:49 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 23 Sep 2025 15:14:14 +0000   Tue, 23 Sep 2025 15:06:49 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 23 Sep 2025 15:14:14 +0000   Tue, 23 Sep 2025 15:06:49 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 23 Sep 2025 15:14:14 +0000   Tue, 23 Sep 2025 15:06:53 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  49586760Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             12142588Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  49586760Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             12142588Ki
  pods:               110
System Info:
  Machine ID:                 0392e032508f4098824a105115e8608a
  System UUID:                4087dd09-9ead-41c2-8858-a8d9ba0ed83b
  Boot ID:                    c127152b-22e2-4bb9-a5fb-f9ba3aa5543b
  Kernel Version:             6.8.0-83-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     django-app-6585df4784-t4cv8         0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m
  default                     django-entrypoint-dxkjd             0 (0%)        0 (0%)      0 (0%)           0 (0%)         10m
  kube-system                 coredns-66bc5c9577-hs9b4            100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     11m
  kube-system                 etcd-minikube                       100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         12m
  kube-system                 kube-apiserver-minikube             250m (6%)     0 (0%)      0 (0%)           0 (0%)         12m
  kube-system                 kube-controller-manager-minikube    200m (5%)     0 (0%)      0 (0%)           0 (0%)         12m
  kube-system                 kube-proxy-dk8cf                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m
  kube-system                 kube-scheduler-minikube             100m (2%)     0 (0%)      0 (0%)           0 (0%)         12m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         12m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%)  0 (0%)
  memory             170Mi (1%)  170Mi (1%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age   From             Message
  ----    ------                   ----  ----             -------
  Normal  Starting                 11m   kube-proxy       
  Normal  Starting                 12m   kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  12m   kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  12m   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    12m   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     12m   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           12m   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.120225] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=421 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=381 
[  +0.120670] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=378 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=338 
[  +0.120997] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=369 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=329 
[  +0.120228] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=449 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=409 
[  +0.120100] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=435 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=395 
[  +0.120259] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=433 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=393 
[  +0.120226] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=421 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=381 
[Sep23 14:54] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:0042:caff:fe80:a61f DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=275 TC=0 HOPLIMIT=10 FLOWLBL=145836 PROTO=UDP SPT=1900 DPT=1900 LEN=235 
[  +0.120649] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:0042:caff:fe80:a61f DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=261 TC=0 HOPLIMIT=10 FLOWLBL=145836 PROTO=UDP SPT=1900 DPT=1900 LEN=221 
[  +0.120323] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:0042:caff:fe80:a61f DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=259 TC=0 HOPLIMIT=10 FLOWLBL=145836 PROTO=UDP SPT=1900 DPT=1900 LEN=219 
[  +0.120880] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:0042:caff:fe80:a61f DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=247 TC=0 HOPLIMIT=10 FLOWLBL=145836 PROTO=UDP SPT=1900 DPT=1900 LEN=207 
[  +0.069128] [UFW BLOCK] IN=vethc3be85f OUT= MAC= SRC=fe80:0000:0000:0000:4496:30ff:fe00:ee7d DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=275 TC=0 HOPLIMIT=10 FLOWLBL=306493 PROTO=UDP SPT=1900 DPT=1900 LEN=235 
[  +0.051385] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:0042:caff:fe80:a61f DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=204 TC=0 HOPLIMIT=10 FLOWLBL=145836 PROTO=UDP SPT=1900 DPT=1900 LEN=164 
[  +0.069332] [UFW BLOCK] IN=vethc3be85f OUT= MAC= SRC=fe80:0000:0000:0000:4496:30ff:fe00:ee7d DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=261 TC=0 HOPLIMIT=10 FLOWLBL=306493 PROTO=UDP SPT=1900 DPT=1900 LEN=221 
[  +0.051359] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:0042:caff:fe80:a61f DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=195 TC=0 HOPLIMIT=10 FLOWLBL=145836 PROTO=UDP SPT=1900 DPT=1900 LEN=155 
[  +0.069373] [UFW BLOCK] IN=vethc3be85f OUT= MAC= SRC=fe80:0000:0000:0000:4496:30ff:fe00:ee7d DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=259 TC=0 HOPLIMIT=10 FLOWLBL=306493 PROTO=UDP SPT=1900 DPT=1900 LEN=219 
[  +0.051277] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:0042:caff:fe80:a61f DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=447 TC=0 HOPLIMIT=10 FLOWLBL=145836 PROTO=UDP SPT=1900 DPT=1900 LEN=407 
[Sep23 14:55] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=449 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=409 
[  +0.121437] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=435 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=395 
[  +0.120343] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=433 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=393 
[Sep23 14:58] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=275 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=235 
[  +0.000256] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=261 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=221 
[  +0.000052] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=259 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=219 
[  +0.000043] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=247 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=207 
[  +0.000041] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=204 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=164 
[  +0.000041] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=195 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=155 
[  +0.001265] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:0042:caff:fe80:a61f DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=275 TC=0 HOPLIMIT=10 FLOWLBL=145836 PROTO=UDP SPT=1900 DPT=1900 LEN=235 
[ +14.456566] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=275 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=235 
[Sep23 15:06] [UFW BLOCK] IN=vetha9a74e4 OUT= MAC= SRC=fe80:0000:0000:0000:b44d:48ff:fe61:b698 DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=275 TC=0 HOPLIMIT=10 FLOWLBL=441906 PROTO=UDP SPT=1900 DPT=1900 LEN=235 
[  +0.120582] [UFW BLOCK] IN=vetha9a74e4 OUT= MAC= SRC=fe80:0000:0000:0000:b44d:48ff:fe61:b698 DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=261 TC=0 HOPLIMIT=10 FLOWLBL=441906 PROTO=UDP SPT=1900 DPT=1900 LEN=221 
[  +0.120284] [UFW BLOCK] IN=vetha9a74e4 OUT= MAC= SRC=fe80:0000:0000:0000:b44d:48ff:fe61:b698 DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=259 TC=0 HOPLIMIT=10 FLOWLBL=441906 PROTO=UDP SPT=1900 DPT=1900 LEN=219 
[  +0.120288] [UFW BLOCK] IN=vetha9a74e4 OUT= MAC= SRC=fe80:0000:0000:0000:b44d:48ff:fe61:b698 DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=247 TC=0 HOPLIMIT=10 FLOWLBL=441906 PROTO=UDP SPT=1900 DPT=1900 LEN=207 
[  +0.120335] [UFW BLOCK] IN=vetha9a74e4 OUT= MAC= SRC=fe80:0000:0000:0000:b44d:48ff:fe61:b698 DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=204 TC=0 HOPLIMIT=10 FLOWLBL=441906 PROTO=UDP SPT=1900 DPT=1900 LEN=164 
[  +0.120637] [UFW BLOCK] IN=vetha9a74e4 OUT= MAC= SRC=fe80:0000:0000:0000:b44d:48ff:fe61:b698 DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=195 TC=0 HOPLIMIT=10 FLOWLBL=441906 PROTO=UDP SPT=1900 DPT=1900 LEN=155 
[  +0.120436] [UFW BLOCK] IN=vetha9a74e4 OUT= MAC= SRC=fe80:0000:0000:0000:b44d:48ff:fe61:b698 DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=449 TC=0 HOPLIMIT=10 FLOWLBL=441906 PROTO=UDP SPT=1900 DPT=1900 LEN=409 
[  +0.125481] [UFW BLOCK] IN=vetha9a74e4 OUT= MAC= SRC=fe80:0000:0000:0000:b44d:48ff:fe61:b698 DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=435 TC=0 HOPLIMIT=10 FLOWLBL=441906 PROTO=UDP SPT=1900 DPT=1900 LEN=395 
[  +0.120327] [UFW BLOCK] IN=vetha9a74e4 OUT= MAC= SRC=fe80:0000:0000:0000:b44d:48ff:fe61:b698 DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=433 TC=0 HOPLIMIT=10 FLOWLBL=441906 PROTO=UDP SPT=1900 DPT=1900 LEN=393 
[  +0.120416] [UFW BLOCK] IN=vetha9a74e4 OUT= MAC= SRC=fe80:0000:0000:0000:b44d:48ff:fe61:b698 DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=421 TC=0 HOPLIMIT=10 FLOWLBL=441906 PROTO=UDP SPT=1900 DPT=1900 LEN=381 
[Sep23 15:08] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=449 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=409 
[  +0.000085] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:0042:caff:fe80:a61f DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=447 TC=0 HOPLIMIT=10 FLOWLBL=145836 PROTO=UDP SPT=1900 DPT=1900 LEN=407 
[  +0.120989] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=435 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=395 
[  +0.000068] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:0042:caff:fe80:a61f DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=433 TC=0 HOPLIMIT=10 FLOWLBL=145836 PROTO=UDP SPT=1900 DPT=1900 LEN=393 
[  +0.120358] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=433 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=393 
[  +0.000102] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:0042:caff:fe80:a61f DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=431 TC=0 HOPLIMIT=10 FLOWLBL=145836 PROTO=UDP SPT=1900 DPT=1900 LEN=391 
[Sep23 15:16] [UFW BLOCK] IN=veth983d676 OUT= MAC= SRC=fe80:0000:0000:0000:fc7b:c5ff:fe63:230d DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=449 TC=0 HOPLIMIT=10 FLOWLBL=652919 PROTO=UDP SPT=1900 DPT=1900 LEN=409 
[  +0.000222] [UFW BLOCK] IN=br-e46b5bc7593f OUT= MAC= SRC=fe80:0000:0000:0000:0042:1aff:fe16:aefc DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=447 TC=0 HOPLIMIT=10 FLOWLBL=11056 PROTO=UDP SPT=1900 DPT=1900 LEN=407 
[  +0.120589] [UFW BLOCK] IN=veth983d676 OUT= MAC= SRC=fe80:0000:0000:0000:fc7b:c5ff:fe63:230d DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=435 TC=0 HOPLIMIT=10 FLOWLBL=652919 PROTO=UDP SPT=1900 DPT=1900 LEN=395 
[  +0.000112] [UFW BLOCK] IN=br-e46b5bc7593f OUT= MAC= SRC=fe80:0000:0000:0000:0042:1aff:fe16:aefc DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=433 TC=0 HOPLIMIT=10 FLOWLBL=11056 PROTO=UDP SPT=1900 DPT=1900 LEN=393 
[  +0.120593] [UFW BLOCK] IN=veth983d676 OUT= MAC= SRC=fe80:0000:0000:0000:fc7b:c5ff:fe63:230d DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=433 TC=0 HOPLIMIT=10 FLOWLBL=652919 PROTO=UDP SPT=1900 DPT=1900 LEN=393 
[  +0.000112] [UFW BLOCK] IN=br-e46b5bc7593f OUT= MAC= SRC=fe80:0000:0000:0000:0042:1aff:fe16:aefc DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=431 TC=0 HOPLIMIT=10 FLOWLBL=11056 PROTO=UDP SPT=1900 DPT=1900 LEN=391 
[  +0.120623] [UFW BLOCK] IN=veth983d676 OUT= MAC= SRC=fe80:0000:0000:0000:fc7b:c5ff:fe63:230d DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=421 TC=0 HOPLIMIT=10 FLOWLBL=652919 PROTO=UDP SPT=1900 DPT=1900 LEN=381 
[  +0.000128] [UFW BLOCK] IN=br-e46b5bc7593f OUT= MAC= SRC=fe80:0000:0000:0000:0042:1aff:fe16:aefc DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=419 TC=0 HOPLIMIT=10 FLOWLBL=11056 PROTO=UDP SPT=1900 DPT=1900 LEN=379 
[  +0.120457] [UFW BLOCK] IN=veth983d676 OUT= MAC= SRC=fe80:0000:0000:0000:fc7b:c5ff:fe63:230d DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=378 TC=0 HOPLIMIT=10 FLOWLBL=652919 PROTO=UDP SPT=1900 DPT=1900 LEN=338 
[  +0.000079] [UFW BLOCK] IN=br-e46b5bc7593f OUT= MAC= SRC=fe80:0000:0000:0000:0042:1aff:fe16:aefc DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=376 TC=0 HOPLIMIT=10 FLOWLBL=11056 PROTO=UDP SPT=1900 DPT=1900 LEN=336 
[Sep23 15:18] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=449 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=409 
[  +0.000079] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:0042:caff:fe80:a61f DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=447 TC=0 HOPLIMIT=10 FLOWLBL=145836 PROTO=UDP SPT=1900 DPT=1900 LEN=407 
[  +0.120520] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=435 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=395 
[  +0.000633] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:0042:caff:fe80:a61f DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=433 TC=0 HOPLIMIT=10 FLOWLBL=145836 PROTO=UDP SPT=1900 DPT=1900 LEN=393 
[  +0.120166] [UFW BLOCK] IN=wlo1 OUT= MAC= SRC=fe80:0000:0000:0000:c6a9:6f2f:9f2a:203a DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=433 TC=0 HOPLIMIT=10 FLOWLBL=882141 PROTO=UDP SPT=1900 DPT=1900 LEN=393 
[  +0.000104] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:0042:caff:fe80:a61f DST=ff02:0000:0000:0000:0000:0000:0000:000c LEN=431 TC=0 HOPLIMIT=10 FLOWLBL=145836 PROTO=UDP SPT=1900 DPT=1900 LEN=391 


==> etcd [5ba7f6f422ea] <==
{"level":"warn","ts":"2025-09-23T15:06:49.879130Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54146","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:49.899964Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54154","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:49.946642Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54174","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:49.974510Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54190","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.004307Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54214","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.030137Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54236","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.055648Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54258","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.118833Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54268","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.148109Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54286","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.173599Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54300","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.202716Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54324","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.239105Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54338","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.270658Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54358","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.308169Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54376","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.431459Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54394","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.493320Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54420","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.507000Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54446","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.524351Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54478","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.571869Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54506","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.603546Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54516","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.608439Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54536","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.631375Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54560","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.646207Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54576","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.683921Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54604","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.784928Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54616","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.878963Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54626","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.941079Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54646","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:50.968022Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54668","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.003652Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54692","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.087925Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54702","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.138523Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54738","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.170912Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54762","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.204842Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54784","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.289997Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54806","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.305020Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54820","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.336266Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54832","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.398872Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54860","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.418149Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54874","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.461952Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54888","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.523535Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54902","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.566123Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54916","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.589371Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54938","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.618257Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54968","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.673209Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54988","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.683927Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55016","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.715941Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55032","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.756867Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55052","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.785823Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55074","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.829001Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55096","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.881556Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55122","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.914464Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55136","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:51.947793Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55164","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-23T15:06:52.077122Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55194","server-name":"","error":"EOF"}
{"level":"info","ts":"2025-09-23T15:16:15.191231Z","caller":"traceutil/trace.go:172","msg":"trace[1710018978] transaction","detail":"{read_only:false; response_revision:979; number_of_response:1; }","duration":"106.150096ms","start":"2025-09-23T15:16:15.085035Z","end":"2025-09-23T15:16:15.191185Z","steps":["trace[1710018978] 'process raft request'  (duration: 105.891058ms)"],"step_count":1}
{"level":"info","ts":"2025-09-23T15:16:49.715254Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":738}
{"level":"info","ts":"2025-09-23T15:16:49.732911Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":738,"took":"17.117556ms","hash":1086008786,"current-db-size-bytes":1884160,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1884160,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-09-23T15:16:49.733072Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1086008786,"revision":738,"compact-revision":-1}
{"level":"warn","ts":"2025-09-23T15:17:35.092900Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"105.361848ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/kubernetes\" limit:1 ","response":"range_response_count:1 size:474"}
{"level":"info","ts":"2025-09-23T15:17:35.093020Z","caller":"traceutil/trace.go:172","msg":"trace[1092603187] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:1042; }","duration":"105.492805ms","start":"2025-09-23T15:17:34.987509Z","end":"2025-09-23T15:17:35.093002Z","steps":["trace[1092603187] 'range keys from in-memory index tree'  (duration: 105.103551ms)"],"step_count":1}
{"level":"info","ts":"2025-09-23T15:17:35.614700Z","caller":"traceutil/trace.go:172","msg":"trace[947057523] transaction","detail":"{read_only:false; response_revision:1043; number_of_response:1; }","duration":"240.254049ms","start":"2025-09-23T15:17:35.374427Z","end":"2025-09-23T15:17:35.614682Z","steps":["trace[947057523] 'process raft request'  (duration: 239.477718ms)"],"step_count":1}


==> kernel <==
 15:19:00 up 1 day, 23:57,  0 users,  load average: 0.74, 0.91, 1.11
Linux minikube 6.8.0-83-generic #83~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Sep  9 18:19:47 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [fd29ba9de78c] <==
I0923 15:06:53.111029       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I0923 15:06:53.111362       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0923 15:06:53.111427       1 default_servicecidr_controller.go:166] Creating default ServiceCIDR with CIDRs: [10.96.0.0/12]
I0923 15:06:53.111651       1 aggregator.go:171] initial CRD sync complete...
I0923 15:06:53.111697       1 autoregister_controller.go:144] Starting autoregister controller
I0923 15:06:53.111723       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0923 15:06:53.111756       1 cache.go:39] Caches are synced for autoregister controller
I0923 15:06:53.115625       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I0923 15:06:53.115753       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0923 15:06:53.120362       1 controller.go:667] quota admission added evaluator for: namespaces
I0923 15:06:53.142653       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0923 15:06:53.150385       1 cache.go:39] Caches are synced for LocalAvailability controller
I0923 15:06:53.150410       1 default_servicecidr_controller.go:228] Setting default ServiceCIDR condition Ready to True
I0923 15:06:53.155257       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0923 15:06:53.157215       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I0923 15:06:53.158367       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0923 15:06:53.160018       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0923 15:06:53.160045       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0923 15:06:53.165113       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0923 15:06:53.185255       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I0923 15:06:53.199184       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0923 15:06:53.203109       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I0923 15:06:53.957000       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0923 15:06:53.964361       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0923 15:06:53.964392       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0923 15:06:54.696065       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0923 15:06:54.760412       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0923 15:06:54.872517       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0923 15:06:54.878710       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0923 15:06:54.880020       1 controller.go:667] quota admission added evaluator for: endpoints
I0923 15:06:54.885292       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0923 15:06:55.082860       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0923 15:06:55.998158       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0923 15:06:56.024093       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0923 15:06:56.049305       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I0923 15:07:00.884897       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0923 15:07:00.893557       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0923 15:07:01.032287       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0923 15:07:01.080547       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I0923 15:08:10.561878       1 controller.go:667] quota admission added evaluator for: jobs.batch
I0923 15:08:14.702505       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0923 15:08:16.181992       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0923 15:08:17.954041       1 alloc.go:328] "allocated clusterIPs" service="default/django-service" clusterIPs={"IPv4":"10.110.21.55"}
I0923 15:09:20.360291       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0923 15:09:27.756471       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0923 15:10:29.329071       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0923 15:10:33.264486       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0923 15:11:36.276214       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0923 15:11:45.659815       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0923 15:12:51.127968       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0923 15:12:51.735839       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0923 15:14:07.303255       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0923 15:14:18.077711       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0923 15:15:27.363337       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0923 15:15:43.290592       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0923 15:16:50.060907       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0923 15:16:53.096974       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0923 15:16:57.541407       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0923 15:18:05.863145       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0923 15:18:20.680806       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [32419c28ac4c] <==
I0923 15:06:59.977311       1 controllermanager.go:781] "Started controller" controller="cronjob-controller"
I0923 15:06:59.977331       1 controllermanager.go:739] "Skipping a cloud provider controller" controller="service-lb-controller"
I0923 15:06:59.977483       1 cronjob_controllerv2.go:145] "Starting cronjob controller v2" logger="cronjob-controller"
I0923 15:06:59.977503       1 shared_informer.go:349] "Waiting for caches to sync" controller="cronjob"
I0923 15:06:59.988920       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I0923 15:07:00.005218       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0923 15:07:00.007252       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I0923 15:07:00.058396       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I0923 15:07:00.064620       1 shared_informer.go:356] "Caches are synced" controller="node"
I0923 15:07:00.064718       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0923 15:07:00.064761       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0923 15:07:00.064817       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I0923 15:07:00.064837       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I0923 15:07:00.065719       1 shared_informer.go:356] "Caches are synced" controller="service account"
I0923 15:07:00.067231       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0923 15:07:00.067257       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I0923 15:07:00.071562       1 shared_informer.go:356] "Caches are synced" controller="expand"
I0923 15:07:00.077217       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I0923 15:07:00.077576       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I0923 15:07:00.078219       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I0923 15:07:00.082473       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I0923 15:07:00.082582       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I0923 15:07:00.083681       1 shared_informer.go:356] "Caches are synced" controller="job"
I0923 15:07:00.084482       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I0923 15:07:00.085447       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I0923 15:07:00.086933       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I0923 15:07:00.087024       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I0923 15:07:00.087357       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0923 15:07:00.088434       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I0923 15:07:00.089881       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I0923 15:07:00.092392       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I0923 15:07:00.092428       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0923 15:07:00.092452       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0923 15:07:00.092492       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I0923 15:07:00.092735       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0923 15:07:00.094963       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I0923 15:07:00.098102       1 shared_informer.go:356] "Caches are synced" controller="GC"
I0923 15:07:00.098238       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I0923 15:07:00.109917       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I0923 15:07:00.112925       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I0923 15:07:00.117309       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I0923 15:07:00.120443       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I0923 15:07:00.129890       1 shared_informer.go:356] "Caches are synced" controller="taint"
I0923 15:07:00.129975       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0923 15:07:00.130113       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0923 15:07:00.130189       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0923 15:07:00.132435       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I0923 15:07:00.132809       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0923 15:07:00.133633       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I0923 15:07:00.134275       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I0923 15:07:00.135489       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0923 15:07:00.135588       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I0923 15:07:00.135740       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I0923 15:07:00.136494       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I0923 15:07:00.137224       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I0923 15:07:00.139183       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I0923 15:07:00.139729       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0923 15:07:00.149811       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I0923 15:07:00.149835       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0923 15:07:00.149844       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-proxy [b773c2b8df2b] <==
I0923 15:07:02.021391       1 server_linux.go:53] "Using iptables proxy"
I0923 15:07:02.093428       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I0923 15:07:02.195463       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I0923 15:07:02.195756       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E0923 15:07:02.195850       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0923 15:07:02.227055       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0923 15:07:02.227112       1 server_linux.go:132] "Using iptables Proxier"
I0923 15:07:02.232314       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0923 15:07:02.240359       1 server.go:527] "Version info" version="v1.34.0"
I0923 15:07:02.240391       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0923 15:07:02.244518       1 config.go:200] "Starting service config controller"
I0923 15:07:02.244543       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I0923 15:07:02.244570       1 config.go:106] "Starting endpoint slice config controller"
I0923 15:07:02.244577       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I0923 15:07:02.244608       1 config.go:403] "Starting serviceCIDR config controller"
I0923 15:07:02.244614       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I0923 15:07:02.247255       1 config.go:309] "Starting node config controller"
I0923 15:07:02.247710       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I0923 15:07:02.247912       1 shared_informer.go:356] "Caches are synced" controller="node config"
I0923 15:07:02.344693       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I0923 15:07:02.344697       1 shared_informer.go:356] "Caches are synced" controller="service config"
I0923 15:07:02.344725       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"


==> kube-scheduler [874a089d62cd] <==
I0923 15:06:50.281427       1 serving.go:386] Generated self-signed cert in-memory
W0923 15:06:53.045103       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0923 15:06:53.045404       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0923 15:06:53.045752       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0923 15:06:53.046021       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0923 15:06:53.162995       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I0923 15:06:53.164744       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0923 15:06:53.176182       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0923 15:06:53.176359       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0923 15:06:53.182110       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0923 15:06:53.182293       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
E0923 15:06:53.195620       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0923 15:06:53.197442       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0923 15:06:53.199646       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0923 15:06:53.201578       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E0923 15:06:53.201676       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0923 15:06:53.205248       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0923 15:06:53.206625       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0923 15:06:53.207740       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0923 15:06:53.209992       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0923 15:06:53.210166       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E0923 15:06:53.214359       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E0923 15:06:53.214767       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0923 15:06:53.214870       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0923 15:06:53.214940       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0923 15:06:53.219875       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0923 15:06:53.220417       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0923 15:06:53.222681       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0923 15:06:53.225384       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0923 15:06:53.232892       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0923 15:06:54.051467       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0923 15:06:54.059951       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0923 15:06:54.149129       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0923 15:06:54.196628       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0923 15:06:54.233861       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E0923 15:06:54.345485       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0923 15:06:54.363897       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0923 15:06:54.391677       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
I0923 15:06:56.777559       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Sep 23 15:13:10 minikube kubelet[2283]: E0923 15:13:10.335902    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:13:14 minikube kubelet[2283]: E0923 15:13:14.336193    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:13:24 minikube kubelet[2283]: E0923 15:13:24.336329    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:13:29 minikube kubelet[2283]: E0923 15:13:29.336262    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:13:37 minikube kubelet[2283]: E0923 15:13:37.335689    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:13:40 minikube kubelet[2283]: E0923 15:13:40.336502    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:13:49 minikube kubelet[2283]: E0923 15:13:49.334860    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:13:58 minikube kubelet[2283]: E0923 15:13:58.339362    2283 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="storeapp:latest"
Sep 23 15:13:58 minikube kubelet[2283]: E0923 15:13:58.339421    2283 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="storeapp:latest"
Sep 23 15:13:58 minikube kubelet[2283]: E0923 15:13:58.339511    2283 kuberuntime_manager.go:1449] "Unhandled Error" err="container django start failed in pod django-app-6585df4784-t4cv8_default(51c4217e-0b5b-4e71-909a-ba9abfe702e4): ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Sep 23 15:13:58 minikube kubelet[2283]: E0923 15:13:58.339551    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ErrImagePull: \"Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:14:02 minikube kubelet[2283]: E0923 15:14:02.335858    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:14:11 minikube kubelet[2283]: E0923 15:14:11.336146    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:14:18 minikube kubelet[2283]: E0923 15:14:18.289963    2283 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="storeapp:latest"
Sep 23 15:14:18 minikube kubelet[2283]: E0923 15:14:18.290012    2283 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="storeapp:latest"
Sep 23 15:14:18 minikube kubelet[2283]: E0923 15:14:18.290089    2283 kuberuntime_manager.go:1449] "Unhandled Error" err="container migrate start failed in pod django-entrypoint-dxkjd_default(04fc6d98-c3ba-4d26-8131-15614642d9b7): ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Sep 23 15:14:18 minikube kubelet[2283]: E0923 15:14:18.290144    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ErrImagePull: \"Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:14:24 minikube kubelet[2283]: E0923 15:14:24.336660    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:14:29 minikube kubelet[2283]: E0923 15:14:29.336195    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:14:38 minikube kubelet[2283]: E0923 15:14:38.353443    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:14:42 minikube kubelet[2283]: E0923 15:14:42.335564    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:14:52 minikube kubelet[2283]: E0923 15:14:52.335543    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:14:57 minikube kubelet[2283]: E0923 15:14:57.335897    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:15:04 minikube kubelet[2283]: E0923 15:15:04.335676    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:15:10 minikube kubelet[2283]: E0923 15:15:10.335731    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:15:15 minikube kubelet[2283]: E0923 15:15:15.336170    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:15:21 minikube kubelet[2283]: E0923 15:15:21.335919    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:15:26 minikube kubelet[2283]: E0923 15:15:26.336844    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:15:35 minikube kubelet[2283]: E0923 15:15:35.335747    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:15:39 minikube kubelet[2283]: E0923 15:15:39.335708    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:15:46 minikube kubelet[2283]: E0923 15:15:46.336273    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:15:53 minikube kubelet[2283]: E0923 15:15:53.335363    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:16:00 minikube kubelet[2283]: E0923 15:16:00.335765    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:16:05 minikube kubelet[2283]: E0923 15:16:05.335723    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:16:15 minikube kubelet[2283]: E0923 15:16:15.335468    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:16:19 minikube kubelet[2283]: E0923 15:16:19.335064    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:16:27 minikube kubelet[2283]: E0923 15:16:27.335220    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:16:30 minikube kubelet[2283]: E0923 15:16:30.352636    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:16:41 minikube kubelet[2283]: E0923 15:16:41.335355    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:16:44 minikube kubelet[2283]: E0923 15:16:44.336958    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:16:53 minikube kubelet[2283]: E0923 15:16:53.335142    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:16:57 minikube kubelet[2283]: E0923 15:16:57.334721    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:17:08 minikube kubelet[2283]: E0923 15:17:08.336674    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:17:10 minikube kubelet[2283]: E0923 15:17:10.336324    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:17:22 minikube kubelet[2283]: E0923 15:17:22.335594    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:17:24 minikube kubelet[2283]: E0923 15:17:24.336160    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:17:35 minikube kubelet[2283]: E0923 15:17:35.335949    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:17:36 minikube kubelet[2283]: E0923 15:17:36.353593    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:17:49 minikube kubelet[2283]: E0923 15:17:49.336286    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:17:49 minikube kubelet[2283]: E0923 15:17:49.336536    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:18:00 minikube kubelet[2283]: E0923 15:18:00.336739    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:18:01 minikube kubelet[2283]: E0923 15:18:01.334866    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:18:15 minikube kubelet[2283]: E0923 15:18:15.335653    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:18:15 minikube kubelet[2283]: E0923 15:18:15.335675    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:18:30 minikube kubelet[2283]: E0923 15:18:30.335948    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:18:30 minikube kubelet[2283]: E0923 15:18:30.336333    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:18:41 minikube kubelet[2283]: E0923 15:18:41.336057    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:18:44 minikube kubelet[2283]: E0923 15:18:44.336305    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"
Sep 23 15:18:52 minikube kubelet[2283]: E0923 15:18:52.335657    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"migrate\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-entrypoint-dxkjd" podUID="04fc6d98-c3ba-4d26-8131-15614642d9b7"
Sep 23 15:18:55 minikube kubelet[2283]: E0923 15:18:55.335332    2283 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django\" with ImagePullBackOff: \"Back-off pulling image \\\"storeapp:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storeapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/django-app-6585df4784-t4cv8" podUID="51c4217e-0b5b-4e71-909a-ba9abfe702e4"


==> storage-provisioner [398f9370eca6] <==
I0923 15:07:01.747566       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0923 15:07:21.446765       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: network is unreachable


==> storage-provisioner [aa60f6b4899f] <==
W0923 15:17:59.905072       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:17:59.911098       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:01.914091       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:01.917543       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:03.923582       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:03.932371       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:05.939293       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:05.951381       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:07.957999       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:07.967850       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:09.975810       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:09.986432       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:11.989432       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:11.994875       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:13.998164       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:14.004474       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:16.011148       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:16.023231       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:18.028609       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:18.036940       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:20.042921       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:20.047684       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:22.053147       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:22.063501       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:24.066308       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:24.069662       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:26.075625       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:26.087754       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:28.094219       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:28.102822       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:30.109419       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:30.117237       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:32.123834       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:32.134851       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:34.142436       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:34.152268       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:36.158871       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:36.168997       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:38.175846       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:38.186106       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:40.190397       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:40.195956       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:42.202647       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:42.213051       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:44.219653       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:44.230248       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:46.236860       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:46.247565       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:48.254195       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:48.265013       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:50.271996       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:50.281723       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:52.288266       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:52.299026       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:54.304343       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:54.315605       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:56.321728       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:56.333567       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:58.340370       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0923 15:18:58.354441       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

